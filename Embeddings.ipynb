{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885d20e8-7672-403f-b906-6103c65f9e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pykeen\n",
      "  Downloading pykeen-1.11.0-py3-none-any.whl.metadata (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: torch in /opt/miniconda3/lib/python3.10/site-packages (2.1.2)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.10/site-packages (2.1.4)\n",
      "Collecting dataclasses-json (from pykeen)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.10/site-packages (from pykeen) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/miniconda3/lib/python3.10/site-packages (from pykeen) (1.11.4)\n",
      "Requirement already satisfied: click in /opt/miniconda3/lib/python3.10/site-packages (from pykeen) (8.1.7)\n",
      "Collecting click-default-group (from pykeen)\n",
      "  Downloading click_default_group-1.2.4-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.10/site-packages (from pykeen) (4.65.0)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.10/site-packages (from pykeen) (2.31.0)\n",
      "Collecting optuna>=2.0.0 (from pykeen)\n",
      "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting tabulate (from pykeen)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting more-click (from pykeen)\n",
      "  Downloading more_click-0.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: more-itertools in /opt/miniconda3/lib/python3.10/site-packages (from pykeen) (10.2.0)\n",
      "Collecting pystow>=0.4.3 (from pykeen)\n",
      "  Downloading pystow-0.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting docdata (from pykeen)\n",
      "  Downloading docdata-0.0.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting class-resolver>=0.5.1 (from pykeen)\n",
      "  Downloading class_resolver-0.5.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/lib/python3.10/site-packages (from pykeen) (6.0.1)\n",
      "Collecting torch-max-mem>=0.1.1 (from pykeen)\n",
      "  Downloading torch_max_mem-0.1.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting torch-ppr>=0.0.7 (from pykeen)\n",
      "  Downloading torch_ppr-0.0.8-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/miniconda3/lib/python3.10/site-packages (from pykeen) (4.7.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/miniconda3/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/miniconda3/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.10/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/miniconda3/lib/python3.10/site-packages (from pandas) (2023.4)\n",
      "Collecting alembic>=1.5.0 (from optuna>=2.0.0->pykeen)\n",
      "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna>=2.0.0->pykeen)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.10/site-packages (from optuna>=2.0.0->pykeen) (23.1)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna>=2.0.0->pykeen)\n",
      "  Downloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->pykeen)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->pykeen)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.10/site-packages (from requests->pykeen) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.10/site-packages (from requests->pykeen) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.10/site-packages (from requests->pykeen) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.10/site-packages (from requests->pykeen) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna>=2.0.0->pykeen)\n",
      "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy>=1.4.2->optuna>=2.0.0->pykeen)\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->pykeen)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading pykeen-1.11.0-py3-none-any.whl (718 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.4/718.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading class_resolver-0.5.4-py3-none-any.whl (29 kB)\n",
      "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pystow-0.6.1-py3-none-any.whl (38 kB)\n",
      "Downloading torch_max_mem-0.1.3-py3-none-any.whl (10 kB)\n",
      "Downloading torch_ppr-0.0.8-py3-none-any.whl (12 kB)\n",
      "Downloading click_default_group-1.2.4-py2.py3-none-any.whl (4.1 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading docdata-0.0.4-py3-none-any.whl (9.1 kB)\n",
      "Downloading more_click-0.1.2-py3-none-any.whl (6.7 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tabulate, mypy-extensions, more-click, marshmallow, Mako, greenlet, docdata, colorlog, click-default-group, class-resolver, typing-inspect, sqlalchemy, pystow, torch-max-mem, dataclasses-json, alembic, torch-ppr, optuna, pykeen\n",
      "Successfully installed Mako-1.3.8 alembic-1.14.0 class-resolver-0.5.4 click-default-group-1.2.4 colorlog-6.9.0 dataclasses-json-0.6.7 docdata-0.0.4 greenlet-3.1.1 marshmallow-3.23.1 more-click-0.1.2 mypy-extensions-1.0.0 optuna-4.1.0 pykeen-1.11.0 pystow-0.6.1 sqlalchemy-2.0.36 tabulate-0.9.0 torch-max-mem-0.1.3 torch-ppr-0.0.8 typing-inspect-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pykeen scikit-learn torch pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ccc43b-1590-41f4-bec1-f97b5c5f79a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fbf22789c60>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pykeen'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpykeen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransE\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpykeen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SoftplusLoss\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpykeen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pykeen'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pykeen.models import TransE\n",
    "from pykeen.losses import SoftplusLoss\n",
    "from pykeen.optimizers import Adam\n",
    "from pykeen.training import TrainingLoop\n",
    "from pykeen.triples import TriplesFactory\n",
    "import os\n",
    "\n",
    "# Load entities, relations, and mapping\n",
    "def load_entities(file_path):\n",
    "    entities = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            entity = json.loads(line.strip())\n",
    "            entities.append(entity['id'])\n",
    "    return entities\n",
    "\n",
    "def load_relations(file_path):\n",
    "    relations = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            relation = json.loads(line.strip())\n",
    "            relations.append((relation['source'], relation['type'], relation['dest']))\n",
    "    return relations\n",
    "\n",
    "def load_values(file_path):\n",
    "    values = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            value = json.loads(line.strip())\n",
    "            values.append((value['id'], value['properties'].get('amount', None), value['properties'].get('value', None)))\n",
    "    return values\n",
    "\n",
    "def load_mapping(file_path):\n",
    "    mapping = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            stock, entity_id = line.strip().split(':')\n",
    "            mapping[stock] = entity_id\n",
    "    return mapping\n",
    "\n",
    "# Folder path\n",
    "folder_path = 'WikiGraph'\n",
    "\n",
    "# Load data\n",
    "entities = load_entities(os.path.join(folder_path, 'entity.txt'))\n",
    "relations = load_relations(os.path.join(folder_path, 'relation.txt'))\n",
    "values = load_values(os.path.join(folder_path, 'Values.txt'))\n",
    "mapping = load_mapping(os.path.join(folder_path, 'mapping.txt'))\n",
    "\n",
    "# Create ID mappings (Entities as strings and relations as strings)\n",
    "entity_to_id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "relation_to_id = {f'P{idx}': idx for idx in range(len(relations))}  # Adjust as needed based on relation identifiers\n",
    "\n",
    "# Prepare triples\n",
    "triples = []\n",
    "\n",
    "# Add relations to triples (Ensure relations are mapped correctly as strings)\n",
    "for relation in relations:\n",
    "    source_id, relation_type, dest_id = relation\n",
    "    if source_id in entity_to_id and dest_id in entity_to_id:\n",
    "        # Ensure that relation_type is in the relation_to_id mapping\n",
    "        relation_id = relation_to_id.get(relation_type, None)\n",
    "        if relation_id is not None:\n",
    "            triples.append((entity_to_id[source_id], relation_id, entity_to_id[dest_id]))\n",
    "\n",
    "# Add values to triples\n",
    "for value in values:\n",
    "    value_id, amount, date = value\n",
    "    if value_id in entity_to_id:\n",
    "        # Here, 'has_value' is a placeholder for the relation, adjust as needed\n",
    "        relation_id = relation_to_id.get('has_value', None)\n",
    "        if relation_id is not None:\n",
    "            triples.append((value_id, relation_id, amount))  # Custom relation for value\n",
    "\n",
    "# Convert triples to tensor\n",
    "triples_tensor = torch.tensor(triples, dtype=torch.long)\n",
    "\n",
    "# Manually create the TriplesFactory with the correct mappings\n",
    "triples_factory = TriplesFactory(\n",
    "    num_entities=len(entity_to_id),\n",
    "    num_relations=len(relation_to_id),\n",
    "    triples=triples_tensor,\n",
    ")\n",
    "\n",
    "# Initialize TransE model\n",
    "model = TransE(\n",
    "    triples_factory=triples_factory,\n",
    "    embedding_dim=100  # Adjust embedding dimension as needed\n",
    ")\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = Adam(model.parameters())\n",
    "loss = SoftplusLoss()\n",
    "\n",
    "# Training loop\n",
    "training_loop = TrainingLoop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    triples_factory=triples_factory,\n",
    "    num_epochs=100,\n",
    "    batch_size=512,  # Adjust batch size as needed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "training_loop.train()\n",
    "\n",
    "# Extract entity and relation embeddings\n",
    "entity_embeddings = model.entity_representations[0].detach().cpu().numpy()\n",
    "relation_embeddings = model.relation_representations[0].detach().cpu().numpy()\n",
    "\n",
    "# Print embeddings\n",
    "print(\"Entity Embeddings:\", entity_embeddings[:5])\n",
    "print(\"Relation Embeddings:\", relation_embeddings[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36b9fc-11a2-447c-ba63-47ef70b11ed9",
   "metadata": {},
   "source": [
    "# Random Forest with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb20765f-62c9-49ff-8553-08113f0c9a68",
   "metadata": {},
   "source": [
    "## Base TIs Modal using WikiData \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435f6d1-d873-4105-943e-53d765adffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DCG and nDCG functions\n",
    "def dcg(scores, k):\n",
    "    scores = scores[:k]\n",
    "    return np.sum([score / np.log2(idx + 2) for idx, score in enumerate(scores)])\n",
    "\n",
    "def ndcg(y_true, y_pred, k=10):\n",
    "    # Sort predictions and true values by predicted scores\n",
    "    sorted_indices = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = np.array(y_true)[sorted_indices]\n",
    "    \n",
    "    # Compute DCG and IDCG\n",
    "    actual_dcg = dcg(y_true_sorted, k)\n",
    "    ideal_dcg = dcg(sorted(y_true, reverse=True), k)\n",
    "    return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "# Load and prepare dataset\n",
    "# Assuming `stock_data` is loaded as a pandas DataFrame\n",
    "# Example: stock_data = pd.read_csv('your_stock_data.csv')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "# Feature engineering\n",
    "X = stock_data[['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21','entity_embeddings','Relation Embeddings:']]\n",
    "y = stock_data['Close']\n",
    "\n",
    "# Filter dates for training and testing\n",
    "train_start_date = '2019-01-01'\n",
    "train_end_date = '2022-05-31'\n",
    "test_start_date = '2022-06-01'\n",
    "test_end_date = '2023-12-31'\n",
    "\n",
    "train_data = stock_data[(stock_data['Date'] >= train_start_date) & (stock_data['Date'] <= train_end_date)]\n",
    "test_data = stock_data[(stock_data['Date'] >= test_start_date) & (stock_data['Date'] <= test_end_date)]\n",
    "\n",
    "# Check for missing values and remove them\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data[['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21','entity_embeddings','Relation Embeddings:']])\n",
    "y_train = train_data['Close'].values\n",
    "X_test = scaler.transform(test_data[['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21','entity_embeddings','Relation Embeddings:']])\n",
    "y_test = test_data['Close'].values\n",
    "\n",
    "# Hyperparameter tuning using RandomizedSearchCV (faster than GridSearchCV)\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_distributions, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model after tuning\n",
    "rf_model = random_search.best_estimator_\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Recompute Metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "ndcg_score = ndcg(y_test, predictions, k=10)\n",
    "\n",
    "# Output Results\n",
    "print(f\"nDCG@10: {ndcg_score}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plot predicted vs actual for a small range of dates\n",
    "date_range = test_data['Date'][:50]  # Select first 50 dates\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(date_range, y_test[:50], label='Actual', marker='o')\n",
    "plt.plot(date_range, predictions[:50], label='Predicted', marker='x')\n",
    "plt.title('Predicted vs Actual Stock Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6018e71-c875-4ce8-a0b7-1d1d43de65fc",
   "metadata": {},
   "source": [
    "## Advance TIs Modal using WikiData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e15dc7-f02f-44ec-9051-69c71f296efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DCG and nDCG functions\n",
    "def dcg(scores, k):\n",
    "    scores = scores[:k]\n",
    "    return np.sum([score / np.log2(idx + 2) for idx, score in enumerate(scores)])\n",
    "\n",
    "def ndcg(y_true, y_pred, k=10):\n",
    "    # Sort predictions and true values by predicted scores\n",
    "    sorted_indices = np.argsort(y_pred)[::-1]\n",
    "    y_true_sorted = np.array(y_true)[sorted_indices]\n",
    "    \n",
    "    # Compute DCG and IDCG\n",
    "    actual_dcg = dcg(y_true_sorted, k)\n",
    "    ideal_dcg = dcg(sorted(y_true, reverse=True), k)\n",
    "    return actual_dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "# Load and prepare dataset\n",
    "# Assuming `stock_data` is loaded as a pandas DataFrame\n",
    "# Example: stock_data = pd.read_csv('your_stock_data.csv')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "# Feature engineering\n",
    "X = stock_data[['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21', 'Volatility', 'Close_Lag1', 'Close_Lag2', 'Return','entity_embeddings','Relation Embeddings:']]\n",
    "y = stock_data['Close']\n",
    "\n",
    "# Filter dates for training and testing\n",
    "train_start_date = '2019-01-01'\n",
    "train_end_date = '2022-05-31'\n",
    "test_start_date = '2022-06-01'\n",
    "test_end_date = '2023-12-31'\n",
    "\n",
    "train_data = stock_data[(stock_data['Date'] >= train_start_date) & (stock_data['Date'] <= train_end_date)]\n",
    "test_data = stock_data[(stock_data['Date'] >= test_start_date) & (stock_data['Date'] <= test_end_date)]\n",
    "\n",
    "# Check for missing values and remove them\n",
    "train_data = train_data.dropna()\n",
    "test_data = test_data.dropna()\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data[['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21', 'Volatility', 'Close_Lag1', 'Close_Lag2', 'Return','entity_embeddings','Relation Embeddings:']])\n",
    "y_train = train_data['Close'].values\n",
    "X_test = scaler.transform(test_data[['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21', 'Volatility', 'Close_Lag1', 'Close_Lag2', 'Return','entity_embeddings','Relation Embeddings:']])\n",
    "y_test = test_data['Close'].values\n",
    "\n",
    "# Hyperparameter tuning using RandomizedSearchCV (faster than GridSearchCV)\n",
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "random_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_distributions, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model after tuning\n",
    "rf_model = random_search.best_estimator_\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Recompute Metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "ndcg_score = ndcg(y_test, predictions, k=10)\n",
    "\n",
    "# Output Results\n",
    "print(f\"nDCG@10: {ndcg_score}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "# Plot predicted vs actual for a small range of dates\n",
    "date_range = test_data['Date'][:50]  # Select first 50 dates\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(date_range, y_test[:50], label='Actual', marker='o')\n",
    "plt.plot(date_range, predictions[:50], label='Predicted', marker='x')\n",
    "plt.title('Predicted vs Actual Stock Prices')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.xticks(rotation=45)\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b66f4-d8eb-498b-8497-fcdfdcaa0cba",
   "metadata": {},
   "source": [
    "# LSTM with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a5daf3-6ade-4621-a2f4-934a9bb4b14b",
   "metadata": {},
   "source": [
    "## Base TIs Modal using WikiData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0526c514-f554-404b-8b8d-316cf4a02972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = ['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21','entity_embeddings','Relation Embeddings:']\n",
    "target = 'Close'\n",
    "\n",
    "# Cap outliers in 'Return' to a reasonable range\n",
    "low, high = stock_data['Return'].quantile(0.01), stock_data['Return'].quantile(0.99)\n",
    "stock_data['Return'] = stock_data['Return'].clip(lower=low, upper=high)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_data = stock_data[(stock_data['Date'] >= '2019-01-01') & (stock_data['Date'] <= '2022-05-31')]\n",
    "test_data = stock_data[(stock_data['Date'] >= '2022-06-01') & (stock_data['Date'] <= '2023-12-31')]\n",
    "\n",
    "# Define separate scalers for features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Fit scalers on training data\n",
    "X_train_scaled = scaler_X.fit_transform(train_data[features])\n",
    "y_train_scaled = scaler_y.fit_transform(train_data[[target]])\n",
    "\n",
    "# Transform test data using the same scalers\n",
    "X_test_scaled = scaler_X.transform(test_data[features])\n",
    "y_test_scaled = scaler_y.transform(test_data[[target]])\n",
    "\n",
    "# Convert to 3D arrays for LSTM\n",
    "time_steps = 10\n",
    "X_train_lstm, y_train_lstm = [], []\n",
    "for i in range(len(X_train_scaled) - time_steps):\n",
    "    X_train_lstm.append(X_train_scaled[i:i+time_steps])\n",
    "    y_train_lstm.append(y_train_scaled[i+time_steps])\n",
    "X_train_lstm, y_train_lstm = np.array(X_train_lstm), np.array(y_train_lstm)\n",
    "\n",
    "X_test_lstm, y_test_lstm = [], []\n",
    "for i in range(len(X_test_scaled) - time_steps):\n",
    "    X_test_lstm.append(X_test_scaled[i:i+time_steps])\n",
    "    y_test_lstm.append(y_test_scaled[i+time_steps])\n",
    "X_test_lstm, y_test_lstm = np.array(X_test_lstm), np.array(y_test_lstm)\n",
    "\n",
    "# Debugging: Check shapes\n",
    "print(f\"Training data shape: {X_train_lstm.shape}, Training target shape: {y_train_lstm.shape}\")\n",
    "print(f\"Testing data shape: {X_test_lstm.shape}, Testing target shape: {y_test_lstm.shape}\")\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True, input_shape=(time_steps, X_train_lstm.shape[2])),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_data=(X_test_lstm, y_test_lstm),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot the training and validation loss over epochs\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test_lstm, y_test_lstm, verbose=0)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions using the LSTM model\n",
    "predictions_scaled = model.predict(X_test_lstm)\n",
    "\n",
    "# Inverse transform predictions and actual values\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "y_test_original = scaler_y.inverse_transform(y_test_lstm)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, predictions))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# Calculate nDCG@10\n",
    "def ndcg_at_k(actual, predicted, k=10):\n",
    "    actual_sorted = sorted(actual, reverse=True)[:k]\n",
    "    dcg = sum([pred / np.log2(idx + 2) for idx, pred in enumerate(predicted[:k])])\n",
    "    idcg = sum([rel / np.log2(idx + 2) for idx, rel in enumerate(actual_sorted)])\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "actual_top_10 = y_test_original[:10].flatten()\n",
    "predicted_top_10 = predictions[:10].flatten()\n",
    "ndcg = ndcg_at_k(actual_top_10, predicted_top_10, k=10)\n",
    "print(f'nDCG@10: {ndcg}')\n",
    "\n",
    "# Plot predictions vs actuals for a small range\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(100), y_test_original[:100], label='Actual', color='blue')\n",
    "plt.plot(range(100), predictions[:100], label='Predicted', color='red', linestyle='--')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual vs Predicted Stock Prices (Small Range)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb01619-5cbc-42e2-913b-a4ee9ccccbc0",
   "metadata": {},
   "source": [
    "## Advance TIs Modal using WikiData "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c88799-08d2-4f99-a499-2d7f697b6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "features = ['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21', 'Return', 'Volatility', 'Close_Lag1', 'Close_Lag2','entity_embeddings','Relation Embeddings:']\n",
    "target = 'Close'\n",
    "\n",
    "# Cap outliers in 'Return' to a reasonable range\n",
    "low, high = stock_data['Return'].quantile(0.01), stock_data['Return'].quantile(0.99)\n",
    "stock_data['Return'] = stock_data['Return'].clip(lower=low, upper=high)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_data = stock_data[(stock_data['Date'] >= '2019-01-01') & (stock_data['Date'] <= '2022-05-31')]\n",
    "test_data = stock_data[(stock_data['Date'] >= '2022-06-01') & (stock_data['Date'] <= '2023-12-31')]\n",
    "\n",
    "# Select features for prediction\n",
    "features = ['Open', 'High', 'Low', 'Volume', 'MA7', 'MA21','Volatility', 'Close_Lag1', 'Close_Lag2', 'Return']\n",
    "target = 'Close'\n",
    "\n",
    "# Define separate scalers for features and target\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "# Fit scalers on training data\n",
    "X_train_scaled = scaler_X.fit_transform(train_data[features])\n",
    "y_train_scaled = scaler_y.fit_transform(train_data[[target]])\n",
    "\n",
    "# Transform test data using the same scalers\n",
    "X_test_scaled = scaler_X.transform(test_data[features])\n",
    "y_test_scaled = scaler_y.transform(test_data[[target]])\n",
    "\n",
    "# Convert to 3D arrays for LSTM\n",
    "time_steps = 10\n",
    "X_train_lstm, y_train_lstm = [], []\n",
    "for i in range(len(X_train_scaled) - time_steps):\n",
    "    X_train_lstm.append(X_train_scaled[i:i+time_steps])\n",
    "    y_train_lstm.append(y_train_scaled[i+time_steps])\n",
    "X_train_lstm, y_train_lstm = np.array(X_train_lstm), np.array(y_train_lstm)\n",
    "\n",
    "X_test_lstm, y_test_lstm = [], []\n",
    "for i in range(len(X_test_scaled) - time_steps):\n",
    "    X_test_lstm.append(X_test_scaled[i:i+time_steps])\n",
    "    y_test_lstm.append(y_test_scaled[i+time_steps])\n",
    "X_test_lstm, y_test_lstm = np.array(X_test_lstm), np.array(y_test_lstm)\n",
    "\n",
    "# Debugging: Check shapes\n",
    "print(f\"Training data shape: {X_train_lstm.shape}, Training target shape: {y_train_lstm.shape}\")\n",
    "print(f\"Testing data shape: {X_test_lstm.shape}, Testing target shape: {y_test_lstm.shape}\")\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', return_sequences=True, input_shape=(time_steps, X_train_lstm.shape[2])),\n",
    "    LSTM(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_data=(X_test_lstm, y_test_lstm),\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot the training and validation loss over epochs\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test_lstm, y_test_lstm, verbose=0)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Make predictions using the LSTM model\n",
    "predictions_scaled = model.predict(X_test_lstm)\n",
    "\n",
    "# Inverse transform predictions and actual values\n",
    "predictions = scaler_y.inverse_transform(predictions_scaled)\n",
    "y_test_original = scaler_y.inverse_transform(y_test_lstm)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, predictions))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# Calculate nDCG@10\n",
    "def ndcg_at_k(actual, predicted, k=10):\n",
    "    actual_sorted = sorted(actual, reverse=True)[:k]\n",
    "    dcg = sum([pred / np.log2(idx + 2) for idx, pred in enumerate(predicted[:k])])\n",
    "    idcg = sum([rel / np.log2(idx + 2) for idx, rel in enumerate(actual_sorted)])\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "actual_top_10 = y_test_original[:10].flatten()\n",
    "predicted_top_10 = predictions[:10].flatten()\n",
    "ndcg = ndcg_at_k(actual_top_10, predicted_top_10, k=10)\n",
    "print(f'nDCG@10: {ndcg}')\n",
    "\n",
    "# Plot predictions vs actuals for a small range\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(100), y_test_original[:100], label='Actual', color='blue')\n",
    "plt.plot(range(100), predictions[:100], label='Predicted', color='red', linestyle='--')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.title('Actual vs Predicted Stock Prices (Small Range)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
